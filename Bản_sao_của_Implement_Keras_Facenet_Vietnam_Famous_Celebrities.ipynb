{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bản sao của Implement Keras-Facenet Vietnam Famous Celebrities.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "3sJZ2q7Kbepe",
        "gnXp9Dadw3u3",
        "QB_kEPEUMwzx",
        "-nbjnUk9im7T",
        "OwXfFFBvzO5Y",
        "FOFMGbvbzXsb",
        "qaNI1X_3z5DD",
        "WV-ZZxm60HGI",
        "6NymG3nby2-C",
        "MF8LqT3uPm-b",
        "gjtoHTqYNiuO",
        "D4vjfP8zy4CR",
        "tXGCakzxvScS",
        "M9aEhrZpvQR_",
        "iAGdv3Htn0Nm",
        "GZg_7ld4oSKc",
        "IcZTPJpPNBj4",
        "oQNhMCyEM22n",
        "SFNj-yeSP2Qh",
        "VDL-OraLNQWB",
        "jtwg-6Bffgdc",
        "ALBWbQqdjcYo"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thoadao0301/AI-Project/blob/PhuongNDU/B%E1%BA%A3n_sao_c%E1%BB%A7a_Implement_Keras_Facenet_Vietnam_Famous_Celebrities.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25mD3_sfv2f2"
      },
      "source": [
        "# **Data processing**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avIV83a7D_FZ"
      },
      "source": [
        "## Mount data from drive\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNpgq-U-Dbyb",
        "outputId": "26773ca8-9145-44ad-95ce-03d33aee91ba"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwexrMeFMh5v"
      },
      "source": [
        "## Library"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArYirRluzjNG"
      },
      "source": [
        "Requirement already satisfied: keras_facenet in /usr/local/lib/python3.7/dist-packages (0.3.2)\n",
        "\n",
        "Requirement already satisfied: mtcnn in /usr/local/lib/python3.7/dist-packages (from keras_facenet) (0.1.0)\n",
        "\n",
        "Requirement already satisfied: opencv-python>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from mtcnn->keras_facenet) (4.1.2.30)\n",
        "\n",
        "Requirement already satisfied: keras>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from mtcnn->keras_facenet) (2.4.3)\n",
        "\n",
        "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python>=4.1.0->mtcnn->keras_facenet) (1.19.5)\n",
        "\n",
        "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras>=2.0.0->mtcnn->keras_facenet) (3.13)\n",
        "\n",
        "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras>=2.0.0->mtcnn->keras_facenet) (2.10.0)\n",
        "\n",
        "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras>=2.0.0->mtcnn->keras_facenet) (1.4.1)\n",
        "\n",
        "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py->keras>=2.0.0->mtcnn->keras_facenet) (1.15.0)\n",
        "\n",
        "\n",
        "// Sklearn\n",
        "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.0.1)\n",
        "\n",
        "Requirement already satisfied, skipping upgrade: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1)\n",
        "\n",
        "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.19.5)\n",
        "\n",
        "Requirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (2.1.0)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmgyB_N-7iZ6",
        "outputId": "6fd28e87-400d-4efa-bcef-03eadc5b77c1"
      },
      "source": [
        "!pip install mtcnn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting mtcnn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/67/43/abee91792797c609c1bf30f1112117f7a87a713ebaa6ec5201d5555a73ef/mtcnn-0.1.0-py3-none-any.whl (2.3MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3MB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from mtcnn) (2.4.3)\n",
            "Requirement already satisfied: opencv-python>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from mtcnn) (4.1.2.30)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras>=2.0.0->mtcnn) (1.19.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras>=2.0.0->mtcnn) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras>=2.0.0->mtcnn) (3.1.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras>=2.0.0->mtcnn) (1.4.1)\n",
            "Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py->keras>=2.0.0->mtcnn) (1.5.2)\n",
            "Installing collected packages: mtcnn\n",
            "Successfully installed mtcnn-0.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-lf0xS6FCxI",
        "outputId": "80ce0b9b-2c11-4a3f-dc27-d33c4286c3fb"
      },
      "source": [
        "!pip install -U scikit-learn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting scikit-learn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/eb/a48f25c967526b66d5f1fa7a984594f0bf0a5afafa94a8c4dbc317744620/scikit_learn-0.24.2-cp37-cp37m-manylinux2010_x86_64.whl (22.3MB)\n",
            "\u001b[K     |████████████████████████████████| 22.3MB 1.4MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.0.1)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f7/12/ec3f2e203afa394a149911729357aa48affc59c20e2c1c8297a60f33f133/threadpoolctl-2.1.0-py3-none-any.whl\n",
            "Installing collected packages: threadpoolctl, scikit-learn\n",
            "  Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "Successfully installed scikit-learn-0.24.2 threadpoolctl-2.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-f1ElE4oq-c",
        "outputId": "1c68684d-ee02-4b5f-e767-8842c3b11608"
      },
      "source": [
        "!pip install keras_facenet"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras_facenet\n",
            "  Downloading https://files.pythonhosted.org/packages/6a/c5/6fadf919a86c44b87ba9d8134cc83820b8fa8a98f5c68ff676179e052839/keras-facenet-0.3.2.tar.gz\n",
            "Requirement already satisfied: mtcnn in /usr/local/lib/python3.7/dist-packages (from keras_facenet) (0.1.0)\n",
            "Requirement already satisfied: opencv-python>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from mtcnn->keras_facenet) (4.1.2.30)\n",
            "Requirement already satisfied: keras>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from mtcnn->keras_facenet) (2.4.3)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python>=4.1.0->mtcnn->keras_facenet) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras>=2.0.0->mtcnn->keras_facenet) (1.4.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras>=2.0.0->mtcnn->keras_facenet) (3.1.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras>=2.0.0->mtcnn->keras_facenet) (3.13)\n",
            "Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py->keras>=2.0.0->mtcnn->keras_facenet) (1.5.2)\n",
            "Building wheels for collected packages: keras-facenet\n",
            "  Building wheel for keras-facenet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-facenet: filename=keras_facenet-0.3.2-cp37-none-any.whl size=10386 sha256=2835a239bed4e6eadbbe74e0f85f17a2483d685cb7fce0e032f81d71416e878e\n",
            "  Stored in directory: /root/.cache/pip/wheels/f6/53/9a/36c4b52fd22faf4f710d5047d874655b85a1b2cf77accfb9bd\n",
            "Successfully built keras-facenet\n",
            "Installing collected packages: keras-facenet\n",
            "Successfully installed keras-facenet-0.3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ok5UDyVzEcuI",
        "outputId": "13b2d6ff-247c-4965-89ec-247be6153174"
      },
      "source": [
        "import sys\n",
        "import os, random\n",
        "import numpy as np\n",
        "import tensorflow.compat.v1 as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import mtcnn\n",
        "import pickle\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "# from keras_facenet import FaceNet\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datetime import datetime\n",
        "os.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices'\n",
        "tf.disable_v2_behavior()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GhmSLUpD99R"
      },
      "source": [
        "## Remove images that has more than two faces or can't detect face"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raiTSNj0EbFz"
      },
      "source": [
        "def check_only_one_face(directory_path):\n",
        "    list_sub_dir = os.listdir(directory_path)\n",
        "    # create the detector, using default weights\n",
        "    embedder = FaceNet()\n",
        "    for sub_dir in list_sub_dir:\n",
        "        count = 0\n",
        "        sub_dir_path = os.path.join(directory_path, sub_dir)\n",
        "        print(sub_dir,':')\n",
        "        images = os.listdir(sub_dir_path)\n",
        "        with tqdm(total=len(images), file=sys.stdout) as pbar:\n",
        "\n",
        "            for image in images:\n",
        "                image_path = os.path.join(sub_dir_path, image)\n",
        "                # load image from file\n",
        "                try:\n",
        "                  img = Image.open(image_path)\n",
        "                except:\n",
        "                  try:\n",
        "                    os.remove(image_path)\n",
        "                    count+=1\n",
        "                    continue\n",
        "                  except:\n",
        "                    print('Can\\'t not delete file:', image_path)\n",
        "                    continue\n",
        "\n",
        "                # convert to RGB, if needed\n",
        "                img = img.convert('RGB')\n",
        "                # convert to array\n",
        "                pixels = np.asarray(img)\n",
        "                # detect faces in the image\n",
        "                results = embedder.extract(pixels)\n",
        "                if len(results) != 1:\n",
        "                    try:\n",
        "                      os.remove(image_path)\n",
        "                      count+=1\n",
        "                    except:\n",
        "                      print('Can\\'t not delete file:', image_path)\n",
        "                pbar.update(1)\n",
        "        print('DELETED: ', count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "YYd0-X7noIHo",
        "outputId": "346945a7-e4dc-46fd-8dd7-40f199307c66"
      },
      "source": [
        "directory_path = '/content/drive/Shareddrives/AI_Project/DatasetVietNam/DataVietNam/VietNam/VN_Male'\n",
        "check_only_one_face(directory_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-5ef299cabdd8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdirectory_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/Shareddrives/AI_Project/DatasetVietNam/DataVietNam/VietNam/VN_Male'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcheck_only_one_face\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-19-f7721245b0bc>\u001b[0m in \u001b[0;36mcheck_only_one_face\u001b[0;34m(directory_path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcheck_only_one_face\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mlist_sub_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;31m# create the detector, using default weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0membedder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFaceNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msub_dir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist_sub_dir\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/Shareddrives/AI_Project/DatasetVietNam/DataVietNam/VietNam/VN_Male'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48zzCgOdoIs6"
      },
      "source": [
        "directory_path = '/content/drive/Shareddrives/AI_Project/DatasetVietNam/DataVietNam/VietNam/VN_Female'\n",
        "check_only_one_face(directory_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sJZ2q7Kbepe"
      },
      "source": [
        "## Create MTCNN face data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4IdeOJdigcW"
      },
      "source": [
        "def extract_face(filename,detector,required_size=(160, 160)):\n",
        "    # load image from file\n",
        "    image = Image.open(filename)\n",
        "    # convert to RGB, if needed\n",
        "    image = image.convert('RGB')\n",
        "    # convert to array\n",
        "    pixels = np.asarray(image)\n",
        "    # create the detector, using default weights\n",
        "    results = detector.detect_faces(pixels)\n",
        "    # extract the bounding box from the first face\n",
        "    x1, y1, width, height = results[0]['box']\n",
        "    \n",
        "    # bug fix\n",
        "    x1, y1 = abs(x1), abs(y1)\n",
        "    x2, y2 = x1 + width, y1 + height\n",
        "    # extract the face\n",
        "    face = pixels[y1:y2, x1:x2]\n",
        "    # resize pixels to the model size\n",
        "    image = Image.fromarray(face)\n",
        "    face_img = image.resize(required_size)\n",
        "\n",
        "    return face_img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDe3MLbEFwQt"
      },
      "source": [
        "def create_mtcnn_dataset(src,des,detector, overwrite=False):\n",
        "    sub_directories = os.listdir(src)\n",
        "    count = 1\n",
        "    n = len(sub_directories)\n",
        "    for sub_dir in sub_directories:\n",
        "\n",
        "        sub_dir_src = os.path.join(src,sub_dir)\n",
        "        sub_dir_des = os.path.join(des,sub_dir)\n",
        "\n",
        "        try:\n",
        "            os.mkdir(sub_dir_des)\n",
        "        except:\n",
        "            print('ERROR ON CREATING FOLDER', sub_dir_des)\n",
        "\n",
        "        image_path_src = os.listdir(sub_dir_src)\n",
        "        print('Loop:' , count,'/',n, ' ',sub_dir,':')\n",
        "\n",
        "        with tqdm(total=len(image_path_src), file=sys.stdout) as pbar: \n",
        "            for image_path in image_path_src:\n",
        "\n",
        "                image_path_src = os.path.join(sub_dir_src,image_path)\n",
        "                image_path_des = os.path.join(sub_dir_des,image_path)\n",
        "\n",
        "                check_exists = os.path.exists(image_path_des)\n",
        "                if check_exists and overwrite:\n",
        "                    face_img = extract_face(image_path_src,detector=detector)\n",
        "                    face_img.save(image_path_des)\n",
        "                    pbar.update(1)\n",
        "                    continue\n",
        "                \n",
        "                if not check_exists:\n",
        "                    face_img = extract_face(image_path_src,detector=detector)\n",
        "                    face_img.save(image_path_des)\n",
        "                pbar.update(1)\n",
        "\n",
        "        count+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJ0cjUpwNxWK"
      },
      "source": [
        "detector = mtcnn.MTCNN()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0GcGRqqsQHR"
      },
      "source": [
        "# **Training the model FaceNet using triplet loss/softmax**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZ6hKNuByJIB"
      },
      "source": [
        "## **Using https://github.com/davidsandberg/facenet for create train dataset** \n",
        "\n",
        "Code for implementing facenet by David Sandberg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwXfFFBvzO5Y"
      },
      "source": [
        "### Install scipy version 1.1.0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEhCOv0qJR8b"
      },
      "source": [
        "!pip install scipy==1.1.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aI6fv-wmNgts"
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from drive.Shareddrives.AI_Project.facenet.src import facenet\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import argparse\n",
        "import sys\n",
        "import pickle\n",
        "from scipy import misc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOFMGbvbzXsb"
      },
      "source": [
        "### Clone FaceNet model and set up enviroment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVEatpEo8dsD"
      },
      "source": [
        "# %%bash\n",
        "# cd /content/drive/Shareddrives/AI_Project\n",
        "# git clone https://github.com/davidsandberg/facenet.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rpRhAuqRBSP"
      },
      "source": [
        "%set_env PYTHONPATH=/content/drive/Shareddrives/AI_Project/facenet/src:/env/python"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qaNI1X_3z5DD"
      },
      "source": [
        "### Create dataset with mtcnn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_BlX0T7TuT3"
      },
      "source": [
        "# Code for fix bug\n",
        "# replace line 85 with this for fix code - detect_face.py\n",
        "# # save np.load\n",
        "# np_load_old = np.load\n",
        "\n",
        "# # modify the default parameters of np.load\n",
        "# np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n",
        "\n",
        "# # call load_data with allow_pickle implicitly set to true\n",
        "# data_dict = np.load(data_path, encoding='latin1').item() #pylint: disable=no-member\n",
        "# # restore np.load for future normal usage\n",
        "# np.load = np_load_old\n",
        "\n",
        "!python /content/drive/Shareddrives/AI_Project/facenet/src/align/align_dataset_mtcnn.py /content/drive/Shareddrives/AI_Project/DataVietNam/mtcnn_dataset_182_female_full/VN_Female /content/drive/Shareddrives/AI_Project/DataVietNam/mtcnn_dataset_182_female_full/mtcnn_dataset_182_full --image_size 182 --margin 44"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WV-ZZxm60HGI"
      },
      "source": [
        "### Training model tensorflow 1 with tripletloss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGJ4Ye-M716a"
      },
      "source": [
        "# Code for fix bug\n",
        "# image = tf.to_float(image) # put code between line 118, and 119 in train_tripletloss.py\n",
        "\n",
        "!python /content/drive/Shareddrives/AI_Project/facenet/src/train_tripletloss.py --logs_base_dir /content/drive/Shareddrives/AI_Project/facenet/logs/facenet/ --models_base_dir /content/drive/Shareddrives/AI_Project/model --data_dir /content/drive/Shareddrives/AI_Project/DatasetVietNam/DataVietNam/DavidSandberg/mtcnn_dataset_182_full --image_size 160 --model_def models.inception_resnet_v1 --optimizer ADAM --learning_rate 0.0001 --weight_decay 1e-4 --max_nrof_epochs 500 --people_per_batch 21 --images_per_person 25"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NymG3nby2-C"
      },
      "source": [
        "### Training model tensorflow 1 with softmax"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uF1h8ysPrBaY"
      },
      "source": [
        "!python /content/drive/Shareddrives/AI_Project/facenet/src/train_softmax.py --logs_base_dir /content/drive/Shareddrives/AI_Project/facenet/logs/facenet/ --models_base_dir /content/drive/Shareddrives/AI_Project/model --data_dir /content/drive/Shareddrives/AI_Project/DatasetVietNam/DataVietNam/DavidSandberg/mtcnn_dataset_182_full --image_size 160 --model_def models.inception_resnet_v1 --optimizer ADAM --learning_rate -1 --max_nrof_epochs 150 --keep_probability 0.8 --random_crop --random_flip --use_fixed_image_standardization --learning_rate_schedule_file /content/drive/Shareddrives/AI_Project/facenet/data/learning_rate_schedule_classifier_casia.txt --weight_decay 5e-4 --embedding_size 512 --lfw_distance_metric 1 --lfw_use_flipped_images --lfw_subtract_mean --validation_set_split_ratio 0.05 --validate_every_n_epochs 5 --prelogits_norm_loss_factor 5e-4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MF8LqT3uPm-b"
      },
      "source": [
        "### Train a classifier on own images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSkP1jGkPvPY"
      },
      "source": [
        "!python classifier.py TRAIN C:\\Users\\namph\\Downloads\\Documents\\Study\\AI_Project\\Dataset\\mtcnn_dataset_182_full C:\\Users\\namph\\Downloads\\Documents\\Study\\David_Git\\model\\ C:\\Users\\namph\\Downloads\\Documents\\Study\\David_Git\\model/lfw_classifier.pkl --batch_size 1000 --min_nrof_images_per_class 40 --nrof_train_images_per_class 35 --use_split_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4vjfP8zy4CR"
      },
      "source": [
        "## **Training keras_facenet model, tensorflow 2** ERROR, NOT FIX YET"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXGCakzxvScS"
      },
      "source": [
        "### Create model, dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9-VhjmRcGE3"
      },
      "source": [
        "# Create model FaceNet for training\n",
        "model = FaceNet().model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElIRLtwScYU-"
      },
      "source": [
        "# Print model architecture\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CX45CPyRceJ_"
      },
      "source": [
        "# Create data from directory for training\n",
        "def get_data_train(src):\n",
        "    x , y = [],[]\n",
        "    list_dir = os.listdir(src)\n",
        "    for sub_dir in list_dir:\n",
        "        list_image_path = os.path.join(src,sub_dir)\n",
        "        list_image = os.listdir(list_image_path)\n",
        "\n",
        "        with tqdm(total=len(list_image), file=sys.stdout) as pbar:\n",
        "            for file_path in list_image:\n",
        "                image_path = os.path.join(list_image_path, file_path)\n",
        "                image = Image.open(image_path)\n",
        "                # convert to RGB, if needed\n",
        "                image = image.convert('RGB')\n",
        "                # convert to array\n",
        "                pixels = np.asarray(image)\n",
        "                x.append(pixels)\n",
        "                y.append(sub_dir)\n",
        "                pbar.update(1)\n",
        "    return np.array(x),np.array(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQh8x3lNpodi"
      },
      "source": [
        "# Create dataset for training\n",
        "src = '/content/drive/Shareddrives/AI_Project/DatasetVietNam/DataVietNam/mtcnn_dataset_182_full'\n",
        "x_train_dataset, y_train_dataset = get_data_train(src)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNuVLpIKeKt-"
      },
      "source": [
        "print(x_train_data.shape, len(y_train_data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31NjHsg6lxl9"
      },
      "source": [
        "x_train, x_valid, y_train, y_valid = train_test_split(x_train_dataset, y_train_dataset, test_size=0.2,stratify = y_train_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9aEhrZpvQR_"
      },
      "source": [
        "### Install tensorflow-addons for using Triplet loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etcu0QFAd_27"
      },
      "source": [
        "!pip install -q -U tensorflow-addons"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOws-gVGvgXx"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wA8FIhPPmcvc"
      },
      "source": [
        "mcp = tf.keras.callbacks.ModelCheckpoint(\"/content/drive/Shareddrives/AI_Project/DatasetVietNam/DataVietNam/VietNam/Train_Checkpoint/model_weights.h5\",\n",
        "                      save_best_only=True,save_weights_only=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jL8FwA1jbz2w"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# label encode targets\n",
        "out_encoder = LabelEncoder()\n",
        "out_encoder.fit(y_train)\n",
        "y_train = out_encoder.transform(y_train)\n",
        "y_valid = out_encoder.transform(y_valid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPzAbnLALq2M"
      },
      "source": [
        "# Create data from original data\n",
        "aug = tf.keras.preprocessing.image.ImageDataGenerator(rotation_range=20, zoom_range=0.15,\n",
        "\twidth_shift_range=0.2, height_shift_range=0.2, shear_range=0.15,\n",
        "\thorizontal_flip=True, fill_mode=\"nearest\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uhn4vMtTkdgq"
      },
      "source": [
        "# Create data shuffle with batch size\n",
        "gen_train = tf.data.Dataset.from_tensor_slices((x_train, y_train)).repeat().shuffle(1024).batch(128)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZu78_dTd24H"
      },
      "source": [
        "import tensorflow_addons as tfa\n",
        "\n",
        "# Training model with TripletSemiHardLoss\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
        "    loss=tfa.losses.TripletSemiHardLoss(),\n",
        "    metrics=[\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZctUp-eRLHoF"
      },
      "source": [
        "# Training model with binary_crossentropy\n",
        "model.compile(loss=\"binary_crossentropy\", \n",
        "              optimizer=tf.optimizers.Adam(0.001),\n",
        "\t            metrics=[\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXBhirzrkk_Z"
      },
      "source": [
        "#model.load_weights('/content/drive/Shareddrives/AI_Project/DatasetVietNam/DataVietNam/VietNam/Train_Checkpoint/model_weights.h5')\n",
        "# Training model\n",
        "model.fit(aug.flow(x_train, y_train, batch_size=64),validation_data=(x_valid,y_valid), callbacks=[mcp],steps_per_epoch=x_train.shape[0]//64, epochs=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCi_B72H7RpL"
      },
      "source": [
        "## Wrong prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qyf5j3Hy7RpL"
      },
      "source": [
        "def prepare_test_data(data_dir,mtcnn):\n",
        "  labels = []\n",
        "  X = []\n",
        "  imgs_emd = []\n",
        "  for class_name in os.listdir(data_dir):\n",
        "    image_path = os.path.join(data_dir,class_name)\n",
        "    print(image_path)\n",
        "    list_file = os.listdir(image_path)\n",
        "    with tqdm(total=len(list_file), file=sys.stdout) as pbar:\n",
        "      for image_file in list_file:\n",
        "        imagefile = os.path.join(image_path,image_file)\n",
        "        try:\n",
        "          image = cv2.imread(imagefile)\n",
        "          image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
        "        except:\n",
        "          print('ERROR: cant open')\n",
        "          continue\n",
        "        image = cv2.resize(image,(160,160))\n",
        "        img_emd = embedder.extract(image, threshold=0.95)\n",
        "        if(len(img_emd)<1):\n",
        "            print('ERROR: Can\\'t not detect face: ')\n",
        "            continue\n",
        "        if(len(img_emd)>1):\n",
        "            print('ERROR: More than two face detected: ')\n",
        "            continue\n",
        "        imgs_emd.append(img_emd[0]['embedding'])\n",
        "        X.append(image)\n",
        "        labels.append(class_name)\n",
        "        pbar.update(1)\n",
        "  return X, labels,imgs_emd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztPGpox47RpM"
      },
      "source": [
        "X_image, labels, imgs_emd = get_image_f_a_em('/content/drive/Shareddrives/AI_Project/DatasetVietNam/DataVietNam/VietNam/VN_Male_Test',mtcnn)\n",
        "X = np.array(X)\n",
        "labels = np.array(labels)\n",
        "imgs_emd = np.array(imgs_emd)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IjvUm0Nf7RpM"
      },
      "source": [
        "def show_wrong_predict(X_image, labels, imgs_emd,model):\n",
        "  labels_encode = out_encoder(labels)\n",
        "  y_preds = model.predict(imgs_emd)\n",
        "  idx_diff = np.flatnonzero(np.array(y_preds) != np.array(labels_encode))\n",
        "  num_wrong_predict = len(idx_diff)\n",
        "  print('Number of wrong predict in test set: ', )\n",
        "  # display image and labels\n",
        "  plt.figure(figsize=(15,15))\n",
        "  \n",
        "  for i in range(0,num_wrong_predic if num_wrong_predic < 10 else 10):\n",
        "    plt.subplot(331+i)\n",
        "    plt.imshow(X_image_[idx_diff[i]])\n",
        "    plt.title(labels[idx_diff[i]] + '/ ' + out_encoder.inverse_transform(y_preds[idx_diff[i]])\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g06n4MeM7RpM"
      },
      "source": [
        "show_wrong_predict(X_image,labels,imga_emd,model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--pHxdWnJ-T0"
      },
      "source": [
        "# **DEMO**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pj2a6YiuP7SW"
      },
      "source": [
        "## Library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lse2eAH6wS0V"
      },
      "source": [
        "# !pip install mtcnn\n",
        "# !pip install -U scikit-learn\n",
        "# !pip install scipy==1.1.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwPlLUcSO9H5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "930cac01-0a0b-4637-87b6-c0543aefec98"
      },
      "source": [
        "import sys\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import mtcnn\n",
        "import pickle\n",
        "import facenet\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "from scipy import misc\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\users\\namph\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcZTPJpPNBj4"
      },
      "source": [
        "## Extract faces data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LiM07JB-NBj5"
      },
      "source": [
        "def extract_faces(img_array,detector,image_size=160,margin=44):\n",
        "    faces_list,bbox = [],[]\n",
        "    # load image from file\n",
        "    img = np.array(img_array)\n",
        "    if img.ndim<2:\n",
        "        return faces_list, bbox\n",
        "    img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    results = detector.detect_faces(img)\n",
        "    # extract the bounding box from the first face\n",
        "    for face in results:\n",
        "        confidence = face['confidence']\n",
        "        if confidence <0.8:\n",
        "            continue\n",
        "        face_bbox = face['box']\n",
        "        x1, y1, width, height = face_bbox\n",
        "        \n",
        "        # bug fix\n",
        "        x1, y1 = abs(x1), abs(y1)\n",
        "        x2, y2 = x1 + width, y1 + height\n",
        "\n",
        "        x1 = x1-margin/2 if x1-margin/2>0 else 0\n",
        "        y1 = y1-margin/2 if y1-margin/2>0 else 0\n",
        "        x2 = x2+margin/2 if x2+margin/2<img.shape[1] else img.shape[1]\n",
        "        y2 = y2+margin/2 if y2+margin/2<img.shape[0] else img.shape[0]\n",
        "        x1,x2,y1,y2=int(x1),int(x2),int(y1),int(y2)\n",
        "        # extract the face\n",
        "        face = img[y1:y2, x1:x2]\n",
        "        # resize pixels to the model size\n",
        "        face_img = misc.imresize(face, (image_size, image_size), interp='bilinear')\n",
        "        faces_list.append(face_img)\n",
        "        bbox.append(face_bbox)\n",
        "\n",
        "    return faces_list,bbox"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQNhMCyEM22n"
      },
      "source": [
        "## Get embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HR5ZcNk_M22p"
      },
      "source": [
        "def load_image(img, image_size, do_prewhiten=True):\n",
        "    image = np.zeros((1, image_size, image_size, 3))\n",
        "    if img.ndim == 2:\n",
        "        img = facenet.to_rgb(img)\n",
        "    if do_prewhiten:\n",
        "        img = facenet.prewhiten(img)\n",
        "    img = facenet.crop(img,False, image_size)\n",
        "    img = facenet.flip(img, False)\n",
        "    image[0,:,:,:] = img\n",
        "    return image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5so4vq84M22s"
      },
      "source": [
        "def get_embedding(image_path,image_size=160):\n",
        "    # Get input and output tensors\n",
        "    images_placeholder = tf.get_default_graph().get_tensor_by_name(\"input:0\")\n",
        "    embeddings = tf.get_default_graph().get_tensor_by_name(\"embeddings:0\")\n",
        "    phase_train_placeholder = tf.get_default_graph().get_tensor_by_name(\"phase_train:0\")\n",
        "    images = load_image(image_path, image_size)\n",
        "    feed_dict = {images_placeholder: images, phase_train_placeholder: False}\n",
        "    emb_array= sess.run(embeddings, feed_dict=feed_dict)\n",
        "    return emb_array"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFNj-yeSP2Qh"
      },
      "source": [
        "## Load classfier model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXkV7uhOP2Qi"
      },
      "source": [
        "def load_model_classfier(model_path):\n",
        "    with open(model_path, 'rb') as infile:\n",
        "        (model, class_names) = pickle.load(infile)\n",
        "        \n",
        "    print('Loaded classifier model from file \"%s\"' % model_path)\n",
        "\n",
        "    return model, class_names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDL-OraLNQWB"
      },
      "source": [
        "## Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNE6YO4KNQWC"
      },
      "source": [
        "def draw_bbox(image,bbox,text):\n",
        "  x,y,w,h = bbox\n",
        "  cv2.rectangle(image, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
        "  cv2.putText(image, text,(x, y), \n",
        "              cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 3)\n",
        "  return image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwCb7DxvNQWF"
      },
      "source": [
        "def out_txt(out_file,duration,text):\n",
        "    out_file = open(output_loc + '/list_faces.txt','a')\n",
        "    out_file.write(f'\\nTime({duration}),{text}')\n",
        "    out_file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmSngz6qNQWD"
      },
      "source": [
        "def out_video(images_folder,input_loc,output_loc,fps=15):\n",
        "  # Python code to convert video to audio\n",
        "  import moviepy.editor as *\n",
        "  # Insert Local Video File Path\n",
        "  clip = mp.VideoFileClip(input_loc)\n",
        "  # Insert Local Audio File Path\n",
        "  clip.audio.write_audiofile(output_loc+'audio.wav',codec='pcm_s16le')\n",
        "  videoclip = ImageSequenceClip(output_loc+'images', fps)\n",
        "  audioclip = AudioFileClip(output_loc+'audio.wav')\n",
        "\n",
        "  videoclip.audio = CompositeAudioClip([audioclip])\n",
        "  videoclip.write_videofile(output_loc+\"result_vid.mp4\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnY5lMuXNQWG"
      },
      "source": [
        "def detect_face(image,id,frame_number,fps,model,output_loc,detector,export_video=False,threshold=0.5):\n",
        "  global name_tag\n",
        "  global time_start  \n",
        "  # Get model and class_names\n",
        "  model,class_names = model\n",
        "  # create the detector, using default weights\n",
        "  faces_list,bboxs_list = extract_faces(image,detector,image_size=160)\n",
        "  # loop through each face in detections\n",
        "  if len(faces_list) == 0 and export_video:\n",
        "    image_path = os.path.join(output_loc,'images')\n",
        "    cv2.imwrite(f'{image_path}/{frame_number}.jpg',image)\n",
        "    return\n",
        "  for i in range(len(faces_list)):\n",
        "    # get embedded image\n",
        "    embedded_img = get_embedding(faces_list[i],image_size=160)\n",
        "    \n",
        "    # get coordinates (x,y) and weight, height (w, h) of the bounding box\n",
        "    bbox = bboxs_list[i]\n",
        "\n",
        "    # get predict from model\n",
        "    predictions = model.predict_proba(embedded_img)\n",
        "    class_index = np.argmax(predictions, axis=1)\n",
        "\n",
        "    # get label name\n",
        "    predict_name = class_names[class_index[0]]\n",
        "\n",
        "    # get probability\n",
        "    class_probability = predictions[0,class_index] * 100\n",
        "    threshold*=100\n",
        "\n",
        "    # Accept with the probability > threshold\n",
        "\n",
        "    duration = round(frame_number/fps,2)\n",
        "    time_cur=[int(duration/3600),int((duration%3600)/60),(duration%3600)%60]\n",
        "    \n",
        "    if id != -1:\n",
        "      if class_probability > threshold and class_index == id:\n",
        "        text = f'{predict_name}:{class_probability}'\n",
        "        dura=\"{:0>2}:{:0>2}:{:0>2.0f}\".format(time_cur[0],time_cur[1],time_cur[2])\n",
        "        out_txt(output_loc,dura,text)\n",
        "        image = draw_bbox(image,bbox,text)\n",
        "    elif class_probability > threshold:\n",
        "      text = f'{predict_name}:{class_probability}'\n",
        "      image = draw_bbox(image,bbox,text)\n",
        "      if name_tag==\"\":\n",
        "        name_tag,time_start=predict_names,time_cur\n",
        "      elif (name_tag!=predict_names):\n",
        "        dura=\"{:0>2}:{:0>2}:{:0>2.0f}-{:0>2}:{:0>2}:{:0>2.0f}\".format(time_start[0],time_start[1],time_start[2],time_cur[0],time_cur[1],time_cur[2])\n",
        "        out_txt(output_loc,dura,name_tag)\n",
        "        #reset\n",
        "        name_tag,time_start=predict_names,time_cur\n",
        "  \n",
        "  if export_video:\n",
        "    image_path = os.path.join(output_loc,'images')\n",
        "    cv2.imwrite(f'{image_path}/{frame_number}.jpg',image)\n",
        "  if frame_number==video_length and id == -1: \n",
        "    dura=\"{:0>2}:{:0>2}:{:0>2.0f}-{:0>2}:{:0>2}:{:0>2.0f}\".format(time_start[0],time_start[1],time_start[2],time_cur[0],time_cur[1],time_cur[2])\n",
        "    out_txt(output_loc,dura,name_tag)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vMXT8-mW7cp"
      },
      "source": [
        "## Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yORZpBKIW5Oz",
        "outputId": "0a676148-d676-48fe-cd47-b6fafcad1cf2"
      },
      "source": [
        "#@title Parameter\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "input_loc = 'C:\\\\Users\\\\namph\\\\Downloads\\\\Documents\\\\Study\\\\David_Git\\\\model_female\\\\vidtest\\\\hoangyenchibi.mp4' #@param {type:\"string\"}\n",
        "output_loc = 'C:\\\\Users\\\\namph\\\\Downloads\\\\Documents\\\\Study\\\\David_Git\\\\model_female\\\\vidtest' #@param {type:\"string\"}\n",
        "model_facenet_path = 'C:\\\\Users\\\\namph\\\\Downloads\\\\Documents\\\\Study\\\\David_Git\\\\model_female\\\\train_model' #@param {type:\"string\"}\n",
        "model_classfier_path = 'C:\\\\Users\\\\namph\\\\Downloads\\\\Documents\\\\Study\\\\David_Git\\\\model_female\\\\train_model\\\\my_classifier.pkl' #@param {type:\"string\"}\n",
        "export_video = True #@param [\"False\", \"True\"] {type:\"raw\"}\n",
        "frame_skip = 1 #@param {type:\"slider\", min:-1, max:100, step:1}\n",
        "threshold = 0.1 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "id =  24#@param {type:\"integer\"}\n",
        "#main\n",
        "\n",
        "# Create folder\n",
        "now = datetime.now()\n",
        "dt_string = now.strftime(\"%d%m%Y%H%M%S\")\n",
        "output_loc=os.path.join(output_loc,dt_string)\n",
        "os.mkdir(output_loc)\n",
        "with tf.device('/GPU:0'):\n",
        "  with tf.Graph().as_default():\n",
        "      with tf.Session() as sess:\n",
        "          # Load facenet model\n",
        "          print('Loading feature extraction model')\n",
        "          facenet.load_model(model_facenet_path)\n",
        "\n",
        "          # Create mtcnn model\n",
        "          detector = mtcnn.MTCNN()\n",
        "\n",
        "          # Create model classifer\n",
        "          model,class_names = load_model_classfier(model_classfier_path)\n",
        "\n",
        "          # Read video\n",
        "          cap = cv2.VideoCapture(input_loc)\n",
        "\n",
        "          # Get fpt from video\n",
        "          fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "          # Get frame information\n",
        "          if frame_skip==0:\n",
        "            frame_skip = int(fps)\n",
        "          frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "          frame_number = 0\n",
        "\n",
        "          # Create folder images for export video\n",
        "          if export_video:\n",
        "            images_video = os.path.join(output_loc,'images')\n",
        "            os.mkdir(images_video)\n",
        "          count = frame_skip\n",
        "          print('Processing video...')\n",
        "          with tqdm(total=(frame_count), file=sys.stdout) as pbar:\n",
        "              while cap.isOpened():\n",
        "                # Extract the frame\n",
        "                ret, frame = cap.read()\n",
        "                if ret == True:\n",
        "                    frame_number+=1\n",
        "                    if frame_number == 1 or frame_number == count:\n",
        "                        # Face Detection\n",
        "                        name_tag=\"\"\n",
        "                        time_start=[]\n",
        "                        detect_face(frame,id,frame_number,fps,(model,class_names),output_loc,\n",
        "                                    detector, export_video, threshold)\n",
        "                        count+=frame_skip\n",
        "                    elif export_video:\n",
        "                        img_path = f'{images_video}/{frame_number}.jpg'\n",
        "                        cv2.imwrite(img_path,frame)\n",
        "                else:\n",
        "                  break\n",
        "                pbar.update(1)\n",
        "          cap.release()\n",
        "          print('Successful write .txt file')\n",
        "\n",
        "          # Export video\n",
        "          if export_video:\n",
        "              images_path = os.path.join(output_loc,'images')\n",
        "              print('Rendering video...')\n",
        "              out_video(images_path,input_loc,output_loc,fps)\n",
        "              print('Successful export .mp4 file: ', out_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading feature extraction model\n",
            "Model directory: C:\\Users\\namph\\Downloads\\Documents\\Study\\David_Git\\model_female\\train_model\n",
            "Metagraph file: model-20210602-222317.meta\n",
            "Checkpoint file: model-20210602-222317.ckpt-14\n",
            "INFO:tensorflow:Restoring parameters from C:\\Users\\namph\\Downloads\\Documents\\Study\\David_Git\\model_female\\train_model\\model-20210602-222317.ckpt-14\n",
            "Loaded classifier model from file \"C:\\Users\\namph\\Downloads\\Documents\\Study\\David_Git\\model_female\\train_model\\my_classifier.pkl\"\n",
            "Processing video...\n",
            "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉| 2922/2923 [10:58<00:00,  4.44it/s]\n",
            "Successful write .txt file\n",
            "Rendering video...\n",
            "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2921/2921 [00:35<00:00, 82.95it/s]\n",
            "Successful export .avi file:  C:\\Users\\namph\\Downloads\\Documents\\Study\\David_Git\\model_female\\vidtest\\07062021092656/project.avi\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7SGnH0bU4vH"
      },
      "source": [
        "List id:\n",
        "\n",
        "0 : ['Ali_HoangDuong']\n",
        "\n",
        "1 : ['BD_Tran']\n",
        "\n",
        "2 : ['Bao_Lam']\n",
        "\n",
        "3 : ['Binz']\n",
        "\n",
        "4 : ['Bui_Anh_Tuan']\n",
        "\n",
        "5 : ['Chau_Gia_Kiet']\n",
        "\n",
        "6 : ['Chau_Khai_Phong']\n",
        "\n",
        "7 : ['Chi_Dan']\n",
        "\n",
        "8 : ['Chi_Thien']\n",
        "\n",
        "9 : ['Dai_Nhan']\n",
        "\n",
        "10 : ['Dam_Vinh_Hung']\n",
        "\n",
        "11 : ['Dan_Nguyen']\n",
        "\n",
        "12 : ['Dan_Truong']\n",
        "\n",
        "13 : ['Dao_Ba_Loc']\n",
        "\n",
        "14 : ['Don_Nguyen']\n",
        "\n",
        "15 : ['Duong_Khac_Linh']\n",
        "\n",
        "16 : ['Duy_Khanh']\n",
        "\n",
        "17 : ['Gin_Tuan_Kiet']\n",
        "\n",
        "18 : ['Ha_Anh_Tuan']\n",
        "\n",
        "19 : ['Hamtet_Truong']\n",
        "\n",
        "20 : ['Ho_Quang_Hieu']\n",
        "\n",
        "21 : ['Ho_Trung_Dung']\n",
        "\n",
        "22 : ['Ho_Viet_Trung']\n",
        "\n",
        "23 : ['Hoai_Lam']\n",
        "\n",
        "24 : ['Hoai_Linh']\n",
        "\n",
        "25 : ['Hoang_Rapper']\n",
        "\n",
        "26 : ['Huy_Tran']\n",
        "\n",
        "27 : ['Huynh_Lap']\n",
        "\n",
        "28 : ['Huynh_Phuong']\n",
        "\n",
        "29 : ['Isaac']\n",
        "\n",
        "30 : ['Justatee']\n",
        "\n",
        "31 : ['Kenvin_Khanh']\n",
        "\n",
        "32 : ['Khac_Viet']\n",
        "\n",
        "33 : ['Khanh_Phuong']\n",
        "\n",
        "34 : ['Khuong_Ngoc']\n",
        "\n",
        "35 : ['Kieu_Minh_tuan']\n",
        "\n",
        "36 : ['Kim_Ly']\n",
        "\n",
        "37 : ['Lam_Hung']\n",
        "\n",
        "38 : ['Lam_Truong']\n",
        "\n",
        "39 : ['Lam_Vinh_Hai']\n",
        "\n",
        "40 : ['Long_Nhat']\n",
        "\n",
        "41 : ['Lou_Hoang']\n",
        "\n",
        "42 : ['Luong_Bang_Quang']\n",
        "\n",
        "43 : ['Luong_Manh_Hai']\n",
        "\n",
        "44 : ['Luong_The_Thanh']\n",
        "\n",
        "45 : ['MC_Thanh_Trung']\n",
        "\n",
        "46 : ['Mai_Tai_Phen']\n",
        "\n",
        "47 : ['Minh_Du']\n",
        "\n",
        "48 : ['Minh_Luan']\n",
        "\n",
        "49 : ['Nam_Cuong']\n",
        "\n",
        "50 : ['Ngo_Kien_Huy']\n",
        "\n",
        "51 : ['Nguyen_Khang']\n",
        "\n",
        "52 : ['Nguyen_Phi_Hung']\n",
        "\n",
        "53 : ['Nguyen_Tran_Trung_Quan']\n",
        "\n",
        "54 : ['Nhan_Phuc_Vinh']\n",
        "\n",
        "55 : ['Noo_Phuoc_Thinh']\n",
        "\n",
        "56 : ['Ong_Cao_Thang']\n",
        "\n",
        "57 : ['OnlyC']\n",
        "\n",
        "58 : ['Pham_Hong_Phuoc']\n",
        "\n",
        "59 : ['Pham_Truong']\n",
        "\n",
        "60 : ['Phan_Anh']\n",
        "\n",
        "61 : ['Phan_Manh_Quynh']\n",
        "\n",
        "62 : ['Quang_Ha']\n",
        "\n",
        "63 : ['Quang_Le']\n",
        "\n",
        "64 : ['Quang_Trung_Tran']\n",
        "\n",
        "65 : ['Quy_Binh']\n",
        "\n",
        "66 : ['Quyen_Linh']\n",
        "\n",
        "67 : ['Rocker_Nguyen']\n",
        "\n",
        "68 : ['ST']\n",
        "\n",
        "69 : ['Son_Tung']\n",
        "\n",
        "70 : ['Soobin_HoangSon']\n",
        "\n",
        "71 : ['TIM']\n",
        "\n",
        "72 : ['Ta_Quang_Thang']\n",
        "\n",
        "73 : ['Thai_Vu']\n",
        "\n",
        "74 : ['Thanh_Bach']\n",
        "\n",
        "75 : ['Thanh_Duy']\n",
        "\n",
        "76 : ['Thanh_Loc']\n",
        "\n",
        "77 : ['Tien_Luat']\n",
        "\n",
        "78 : ['Touliver']\n",
        "\n",
        "79 : ['Tran_Thanh']\n",
        "\n",
        "80 : ['Tran_Tien_Dat']\n",
        "\n",
        "81 : ['Trinh_Dinh_Quang']\n",
        "\n",
        "82 : ['Trinh_Thang_Binh']\n",
        "\n",
        "83 : ['Trong_Hieu']\n",
        "\n",
        "84 : ['Truc_Nhan']\n",
        "\n",
        "85 : ['Trung_Quan']\n",
        "\n",
        "86 : ['Truong_Giang']\n",
        "\n",
        "87 : ['Truong_Nam_Thanh']\n",
        "\n",
        "88 : ['Truong_The_Vinh']\n",
        "\n",
        "89 : ['Truong_Vu']\n",
        "\n",
        "90 : ['Tu_Long']\n",
        "\n",
        "91 : ['Tuan_Hung']\n",
        "\n",
        "92 : ['Tung_Duong']\n",
        "\n",
        "93 : ['Ung_Dai_Ve']\n",
        "\n",
        "94 : ['Ung_Hoang_Phuc']\n",
        "\n",
        "95 : ['Vinh_Rau']\n",
        "\n",
        "96 : ['Will']\n",
        "\n",
        "97 : ['Xuan_Bac']\n",
        "\n",
        "98 : ['Xuan_Nghi']\n",
        "\n",
        "99 : ['chubin']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJ6Qfs6136xx"
      },
      "source": [
        "List id female:\n",
        "0 : An Nguy\n",
        "\n",
        "1 : Angela Phuong Trinh\n",
        "\n",
        "2 : Bao Anh\n",
        "\n",
        "3 : Bao Thi\n",
        "\n",
        "4 : Bich Phuong\n",
        "\n",
        "5 : Cam Ly\n",
        "\n",
        "6 : Chi Pu\n",
        "\n",
        "7 : Diep Lam Anh\n",
        "\n",
        "8 : Dieu Nhi\n",
        "\n",
        "9 : Dinh Huong\n",
        "\n",
        "10 : Dinh Ngoc Diep\n",
        "\n",
        "11 : Dong Nhi\n",
        "\n",
        "12 : Elly Tran\n",
        "\n",
        "13 : Emily\n",
        "\n",
        "14 : Giang Hong Ngoc\n",
        "\n",
        "15 : Han Sara\n",
        "\n",
        "16 : Hariwon\n",
        "\n",
        "17 : Hien Ho\n",
        "\n",
        "18 : Hien Thuc\n",
        "\n",
        "19 : Ho Ngoc Ha\n",
        "\n",
        "20 : Hoa MinZy\n",
        "\n",
        "21 : Hoang Oanh\n",
        "\n",
        "22 : Hoang Thuy\n",
        "\n",
        "23 : Hoang Thuy Linh\n",
        "\n",
        "24 : Hoang Yen Chipi\n",
        "\n",
        "25 : Huong Giang idol\n",
        "\n",
        "26 : Huong Tram\n",
        "\n",
        "27 : Huyen Anh\n",
        "\n",
        "28 : JangMi\n",
        "\n",
        "29 : Jennifer Pham\n",
        "\n",
        "30 : Jun Pham\n",
        "\n",
        "31 : Jun Vu\n",
        "\n",
        "32 : Katty Nguyen\n",
        "\n",
        "33 : Kha Ngan\n",
        "\n",
        "34 : Khoi My\n",
        "\n",
        "35 : Khong Tu Quynh\n",
        "\n",
        "36 : Kieu Trinh\n",
        "\n",
        "37 : Kiko Chang\n",
        "\n",
        "38 : Kim Cuong\n",
        "\n",
        "39 : Lan Khue\n",
        "\n",
        "40 : Le Giang\n",
        "\n",
        "41 : Luu De Ly\n",
        "\n",
        "42 : Ly Nha Ky\n",
        "\n",
        "43 : MIN\n",
        "\n",
        "44 : Ma Bich Tram\n",
        "\n",
        "45 : Mai Ho\n",
        "\n",
        "46 : Mai Phuong Thuy\n",
        "\n",
        "47 : Mai Thien Van\n",
        "\n",
        "48 : Mai Tuyet Tran\n",
        "\n",
        "49 : Midu\n",
        "\n",
        "50 : Minh Hang\n",
        "\n",
        "51 : Minh Tuyet\n",
        "\n",
        "52 : Miss Thy\n",
        "\n",
        "53 : Miu Le\n",
        "\n",
        "54 : Mon 2k\n",
        "\n",
        "55 : My Linh\n",
        "\n",
        "56 : My Tam\n",
        "\n",
        "57 : Nam Em\n",
        "\n",
        "58 : Ngan Khanh\n",
        "\n",
        "59 : Nghi Dinh\n",
        "\n",
        "60 : Ngoc Trinh\n",
        "\n",
        "61 : Nguyen Cao Ky Duyen\n",
        "\n",
        "62 : Nha Phuong\n",
        "\n",
        "63 : Ninh Duong Lan Ngoc\n",
        "\n",
        "64 : Oc Thanh Van\n",
        "\n",
        "65 : Pham Huong\n",
        "\n",
        "66 : Pham My Linh\n",
        "\n",
        "67 : Phung Khac Linh\n",
        "\n",
        "68 : Phuong Ly\n",
        "\n",
        "69 : Phuong My Chi\n",
        "\n",
        "70 : Phuong Ngan Nguyen\n",
        "\n",
        "71 : Phuong Thanh\n",
        "\n",
        "72 : Que Van\n",
        "\n",
        "73 : Quynh Kool\n",
        "\n",
        "74 : Ribi Sachi\n",
        "\n",
        "75 : Saka Truong Tuyen\n",
        "\n",
        "76 : Sam\n",
        "\n",
        "77 : Tam Tit\n",
        "\n",
        "78 : Tang Thien Kim\n",
        "\n",
        "79 : Thai Tuyet Tram\n",
        "\n",
        "80 : Thieu Bao Tram\n",
        "\n",
        "81 : Thu Trang\n",
        "\n",
        "82 : Thuy Chi\n",
        "\n",
        "83 : Thuy Dung Dao\n",
        "\n",
        "84 : Toc Tien\n",
        "\n",
        "85 : Tran Thao Hien\n",
        "\n",
        "86 : Trang Hy\n",
        "\n",
        "87 : Trang Khieu\n",
        "\n",
        "88 : Truong My Nhan\n",
        "\n",
        "89 : Truong Quynh Anh\n",
        "\n",
        "90 : Uyen Linh\n",
        "\n",
        "91 : Van Mai Huong\n",
        "\n",
        "92 : Van Trang\n",
        "\n",
        "93 : Vi Oanh\n",
        "\n",
        "94 : Viet Huong\n",
        "\n",
        "95 : Vu Cat Tuong\n",
        "\n",
        "96 : Wendy Thao"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtwg-6Bffgdc"
      },
      "source": [
        "# **VGG-Face**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Z7sC8FcgF3K"
      },
      "source": [
        "pip install keras-vggface"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRBVuzBwgRa2"
      },
      "source": [
        "pip install mtcnn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OymhEc0libAL"
      },
      "source": [
        "from matplotlib import pyplot\n",
        "from PIL import Image\n",
        "from numpy import asarray\n",
        "from numpy import expand_dims\n",
        "from scipy.spatial.distance import cosine\n",
        "from mtcnn.mtcnn import MTCNN\n",
        "from keras_vggface.vggface import VGGFace\n",
        "from keras_vggface.utils import preprocess_input\n",
        "from keras_vggface.utils import decode_predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnOku9rfhj6n"
      },
      "source": [
        "# extract a single face from a given photograph\n",
        "def extract_face(filename, required_size=(224, 224)):\n",
        "\t# load image from file\n",
        "\tpixels = pyplot.imread(filename)\n",
        "\t# create the detector, using default weights\n",
        "\tdetector = MTCNN()\n",
        "\t# detect faces in the image\n",
        "\tresults = detector.detect_faces(pixels)\n",
        "\t# extract the bounding box from the first face\n",
        "\tx1, y1, width, height = results[0]['box']\n",
        "\tx2, y2 = x1 + width, y1 + height\n",
        "\t# extract the face\n",
        "\tface = pixels[y1:y2, x1:x2]\n",
        "\t# resize pixels to the model size\n",
        "\timage = Image.fromarray(face)\n",
        "\timage = image.resize(required_size)\n",
        "\tface_array = asarray(image)\n",
        "\treturn face_array"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pC-V8WqcgiMc"
      },
      "source": [
        "# Example of face detection with a vggface2 model \n",
        "# load the photo and extract the face\n",
        "pixels = extract_face('sharon_stone1.jpg')\n",
        "# convert one face into samples\n",
        "pixels = pixels.astype('float32')\n",
        "samples = expand_dims(pixels, axis=0)\n",
        "# prepare the face for the model, e.g. center pixels\n",
        "samples = preprocess_input(samples, version=2)\n",
        "# create a vggface model\n",
        "model = VGGFace(model='resnet50')\n",
        "# perform prediction\n",
        "yhat = model.predict(samples)\n",
        "# convert prediction into names\n",
        "results = decode_predictions(yhat)\n",
        "# display most likely results\n",
        "for result in results[0]:\n",
        "\tprint('%s: %.3f%%' % (result[0], result[1]*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQ9SFKkTiXrB"
      },
      "source": [
        "# extract faces and calculate face embeddings for a list of photo files\n",
        "def get_embeddings(filenames):\n",
        "\t# extract faces\n",
        "\tfaces = [extract_face(f) for f in filenames]\n",
        "\t# convert into an array of samples\n",
        "\tsamples = asarray(faces, 'float32')\n",
        "\t# prepare the face for the model, e.g. center pixels\n",
        "\tsamples = preprocess_input(samples, version=2)\n",
        "\t# create a vggface model\n",
        "\tmodel = VGGFace(model='resnet50', include_top=False, input_shape=(224, 224, 3), pooling='avg')\n",
        "\t# perform prediction\n",
        "\tyhat = model.predict(samples)\n",
        "\treturn yhat\n",
        "\n",
        "# determine if a candidate face is a match for a known face\n",
        "def is_match(known_embedding, candidate_embedding, thresh=0.5):\n",
        "\t# calculate distance between embeddings\n",
        "\tscore = cosine(known_embedding, candidate_embedding)\n",
        "\tif score <= thresh:\n",
        "\t\tprint('>face is a Match (%.3f <= %.3f)' % (score, thresh))\n",
        "\telse:\n",
        "\t\tprint('>face is NOT a Match (%.3f > %.3f)' % (score, thresh))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6kI92WRg8Iv"
      },
      "source": [
        "# face verification with the VGGFace2 model\n",
        "# define filenames\n",
        "filenames = ['sharon_stone1.jpg', 'sharon_stone2.jpg',\n",
        "\t'sharon_stone3.jpg', 'channing_tatum.jpg']\n",
        "# get embeddings file filenames\n",
        "embeddings = get_embeddings(filenames)\n",
        "# define sharon stone\n",
        "sharon_id = embeddings[0]\n",
        "# verify known photos of sharon\n",
        "print('Positive Tests')\n",
        "is_match(embeddings[0], embeddings[1])\n",
        "is_match(embeddings[0], embeddings[2])\n",
        "# verify known photos of other people\n",
        "print('Negative Tests')\n",
        "is_match(embeddings[0], embeddings[3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALBWbQqdjcYo"
      },
      "source": [
        "# **OpenFace**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mp3glYMGkkG1"
      },
      "source": [
        "https://github.com/iwantooxxoox/Keras-OpenFace"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SO32CfOPjeNa"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate\n",
        "from keras.models import Model\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers.pooling import MaxPooling2D, AveragePooling2D\n",
        "from keras.layers.merge import Concatenate\n",
        "from keras.layers.core import Lambda, Flatten, Dense\n",
        "from keras.engine.topology import Layer\n",
        "from keras import backend as K\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from numpy import genfromtxt\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from utils import LRN2D\n",
        "import utils\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "np.set_printoptions(threshold=np.nan)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWqkymZ6kInv"
      },
      "source": [
        "myInput = Input(shape=(96, 96, 3))\n",
        "\n",
        "x = ZeroPadding2D(padding=(3, 3), input_shape=(96, 96, 3))(myInput)\n",
        "x = Conv2D(64, (7, 7), strides=(2, 2), name='conv1')(x)\n",
        "x = BatchNormalization(axis=3, epsilon=0.00001, name='bn1')(x)\n",
        "x = Activation('relu')(x)\n",
        "x = ZeroPadding2D(padding=(1, 1))(x)\n",
        "x = MaxPooling2D(pool_size=3, strides=2)(x)\n",
        "x = Lambda(LRN2D, name='lrn_1')(x)\n",
        "x = Conv2D(64, (1, 1), name='conv2')(x)\n",
        "x = BatchNormalization(axis=3, epsilon=0.00001, name='bn2')(x)\n",
        "x = Activation('relu')(x)\n",
        "x = ZeroPadding2D(padding=(1, 1))(x)\n",
        "x = Conv2D(192, (3, 3), name='conv3')(x)\n",
        "x = BatchNormalization(axis=3, epsilon=0.00001, name='bn3')(x)\n",
        "x = Activation('relu')(x)\n",
        "x = Lambda(LRN2D, name='lrn_2')(x)\n",
        "x = ZeroPadding2D(padding=(1, 1))(x)\n",
        "x = MaxPooling2D(pool_size=3, strides=2)(x)\n",
        "\n",
        "# Inception3a\n",
        "inception_3a_3x3 = Conv2D(96, (1, 1), name='inception_3a_3x3_conv1')(x)\n",
        "inception_3a_3x3 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3a_3x3_bn1')(inception_3a_3x3)\n",
        "inception_3a_3x3 = Activation('relu')(inception_3a_3x3)\n",
        "inception_3a_3x3 = ZeroPadding2D(padding=(1, 1))(inception_3a_3x3)\n",
        "inception_3a_3x3 = Conv2D(128, (3, 3), name='inception_3a_3x3_conv2')(inception_3a_3x3)\n",
        "inception_3a_3x3 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3a_3x3_bn2')(inception_3a_3x3)\n",
        "inception_3a_3x3 = Activation('relu')(inception_3a_3x3)\n",
        "\n",
        "inception_3a_5x5 = Conv2D(16, (1, 1), name='inception_3a_5x5_conv1')(x)\n",
        "inception_3a_5x5 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3a_5x5_bn1')(inception_3a_5x5)\n",
        "inception_3a_5x5 = Activation('relu')(inception_3a_5x5)\n",
        "inception_3a_5x5 = ZeroPadding2D(padding=(2, 2))(inception_3a_5x5)\n",
        "inception_3a_5x5 = Conv2D(32, (5, 5), name='inception_3a_5x5_conv2')(inception_3a_5x5)\n",
        "inception_3a_5x5 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3a_5x5_bn2')(inception_3a_5x5)\n",
        "inception_3a_5x5 = Activation('relu')(inception_3a_5x5)\n",
        "\n",
        "inception_3a_pool = MaxPooling2D(pool_size=3, strides=2)(x)\n",
        "inception_3a_pool = Conv2D(32, (1, 1), name='inception_3a_pool_conv')(inception_3a_pool)\n",
        "inception_3a_pool = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3a_pool_bn')(inception_3a_pool)\n",
        "inception_3a_pool = Activation('relu')(inception_3a_pool)\n",
        "inception_3a_pool = ZeroPadding2D(padding=((3, 4), (3, 4)))(inception_3a_pool)\n",
        "\n",
        "inception_3a_1x1 = Conv2D(64, (1, 1), name='inception_3a_1x1_conv')(x)\n",
        "inception_3a_1x1 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3a_1x1_bn')(inception_3a_1x1)\n",
        "inception_3a_1x1 = Activation('relu')(inception_3a_1x1)\n",
        "\n",
        "inception_3a = concatenate([inception_3a_3x3, inception_3a_5x5, inception_3a_pool, inception_3a_1x1], axis=3)\n",
        "\n",
        "# Inception3b\n",
        "inception_3b_3x3 = Conv2D(96, (1, 1), name='inception_3b_3x3_conv1')(inception_3a)\n",
        "inception_3b_3x3 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3b_3x3_bn1')(inception_3b_3x3)\n",
        "inception_3b_3x3 = Activation('relu')(inception_3b_3x3)\n",
        "inception_3b_3x3 = ZeroPadding2D(padding=(1, 1))(inception_3b_3x3)\n",
        "inception_3b_3x3 = Conv2D(128, (3, 3), name='inception_3b_3x3_conv2')(inception_3b_3x3)\n",
        "inception_3b_3x3 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3b_3x3_bn2')(inception_3b_3x3)\n",
        "inception_3b_3x3 = Activation('relu')(inception_3b_3x3)\n",
        "\n",
        "inception_3b_5x5 = Conv2D(32, (1, 1), name='inception_3b_5x5_conv1')(inception_3a)\n",
        "inception_3b_5x5 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3b_5x5_bn1')(inception_3b_5x5)\n",
        "inception_3b_5x5 = Activation('relu')(inception_3b_5x5)\n",
        "inception_3b_5x5 = ZeroPadding2D(padding=(2, 2))(inception_3b_5x5)\n",
        "inception_3b_5x5 = Conv2D(64, (5, 5), name='inception_3b_5x5_conv2')(inception_3b_5x5)\n",
        "inception_3b_5x5 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3b_5x5_bn2')(inception_3b_5x5)\n",
        "inception_3b_5x5 = Activation('relu')(inception_3b_5x5)\n",
        "\n",
        "inception_3b_pool = Lambda(lambda x: x**2, name='power2_3b')(inception_3a)\n",
        "inception_3b_pool = AveragePooling2D(pool_size=(3, 3), strides=(3, 3))(inception_3b_pool)\n",
        "inception_3b_pool = Lambda(lambda x: x*9, name='mult9_3b')(inception_3b_pool)\n",
        "inception_3b_pool = Lambda(lambda x: K.sqrt(x), name='sqrt_3b')(inception_3b_pool)\n",
        "inception_3b_pool = Conv2D(64, (1, 1), name='inception_3b_pool_conv')(inception_3b_pool)\n",
        "inception_3b_pool = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3b_pool_bn')(inception_3b_pool)\n",
        "inception_3b_pool = Activation('relu')(inception_3b_pool)\n",
        "inception_3b_pool = ZeroPadding2D(padding=(4, 4))(inception_3b_pool)\n",
        "\n",
        "inception_3b_1x1 = Conv2D(64, (1, 1), name='inception_3b_1x1_conv')(inception_3a)\n",
        "inception_3b_1x1 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3b_1x1_bn')(inception_3b_1x1)\n",
        "inception_3b_1x1 = Activation('relu')(inception_3b_1x1)\n",
        "\n",
        "inception_3b = concatenate([inception_3b_3x3, inception_3b_5x5, inception_3b_pool, inception_3b_1x1], axis=3)\n",
        "\n",
        "# Inception3c\n",
        "inception_3c_3x3 = utils.conv2d_bn(inception_3b,\n",
        "                                   layer='inception_3c_3x3',\n",
        "                                   cv1_out=128,\n",
        "                                   cv1_filter=(1, 1),\n",
        "                                   cv2_out=256,\n",
        "                                   cv2_filter=(3, 3),\n",
        "                                   cv2_strides=(2, 2),\n",
        "                                   padding=(1, 1))\n",
        "\n",
        "inception_3c_5x5 = utils.conv2d_bn(inception_3b,\n",
        "                                   layer='inception_3c_5x5',\n",
        "                                   cv1_out=32,\n",
        "                                   cv1_filter=(1, 1),\n",
        "                                   cv2_out=64,\n",
        "                                   cv2_filter=(5, 5),\n",
        "                                   cv2_strides=(2, 2),\n",
        "                                   padding=(2, 2))\n",
        "\n",
        "inception_3c_pool = MaxPooling2D(pool_size=3, strides=2)(inception_3b)\n",
        "inception_3c_pool = ZeroPadding2D(padding=((0, 1), (0, 1)))(inception_3c_pool)\n",
        "\n",
        "inception_3c = concatenate([inception_3c_3x3, inception_3c_5x5, inception_3c_pool], axis=3)\n",
        "\n",
        "#inception 4a\n",
        "inception_4a_3x3 = utils.conv2d_bn(inception_3c,\n",
        "                                   layer='inception_4a_3x3',\n",
        "                                   cv1_out=96,\n",
        "                                   cv1_filter=(1, 1),\n",
        "                                   cv2_out=192,\n",
        "                                   cv2_filter=(3, 3),\n",
        "                                   cv2_strides=(1, 1),\n",
        "                                   padding=(1, 1))\n",
        "inception_4a_5x5 = utils.conv2d_bn(inception_3c,\n",
        "                                   layer='inception_4a_5x5',\n",
        "                                   cv1_out=32,\n",
        "                                   cv1_filter=(1, 1),\n",
        "                                   cv2_out=64,\n",
        "                                   cv2_filter=(5, 5),\n",
        "                                   cv2_strides=(1, 1),\n",
        "                                   padding=(2, 2))\n",
        "\n",
        "inception_4a_pool = Lambda(lambda x: x**2, name='power2_4a')(inception_3c)\n",
        "inception_4a_pool = AveragePooling2D(pool_size=(3, 3), strides=(3, 3))(inception_4a_pool)\n",
        "inception_4a_pool = Lambda(lambda x: x*9, name='mult9_4a')(inception_4a_pool)\n",
        "inception_4a_pool = Lambda(lambda x: K.sqrt(x), name='sqrt_4a')(inception_4a_pool)\n",
        "inception_4a_pool = utils.conv2d_bn(inception_4a_pool,\n",
        "                                   layer='inception_4a_pool',\n",
        "                                   cv1_out=128,\n",
        "                                   cv1_filter=(1, 1),\n",
        "                                   padding=(2, 2))\n",
        "inception_4a_1x1 = utils.conv2d_bn(inception_3c,\n",
        "                                   layer='inception_4a_1x1',\n",
        "                                   cv1_out=256,\n",
        "                                   cv1_filter=(1, 1))\n",
        "inception_4a = concatenate([inception_4a_3x3, inception_4a_5x5, inception_4a_pool, inception_4a_1x1], axis=3)\n",
        "\n",
        "#inception4e\n",
        "inception_4e_3x3 = utils.conv2d_bn(inception_4a,\n",
        "                                   layer='inception_4e_3x3',\n",
        "                                   cv1_out=160,\n",
        "                                   cv1_filter=(1, 1),\n",
        "                                   cv2_out=256,\n",
        "                                   cv2_filter=(3, 3),\n",
        "                                   cv2_strides=(2, 2),\n",
        "                                   padding=(1, 1))\n",
        "inception_4e_5x5 = utils.conv2d_bn(inception_4a,\n",
        "                                   layer='inception_4e_5x5',\n",
        "                                   cv1_out=64,\n",
        "                                   cv1_filter=(1, 1),\n",
        "                                   cv2_out=128,\n",
        "                                   cv2_filter=(5, 5),\n",
        "                                   cv2_strides=(2, 2),\n",
        "                                   padding=(2, 2))\n",
        "inception_4e_pool = MaxPooling2D(pool_size=3, strides=2)(inception_4a)\n",
        "inception_4e_pool = ZeroPadding2D(padding=((0, 1), (0, 1)))(inception_4e_pool)\n",
        "\n",
        "inception_4e = concatenate([inception_4e_3x3, inception_4e_5x5, inception_4e_pool], axis=3)\n",
        "\n",
        "#inception5a\n",
        "inception_5a_3x3 = utils.conv2d_bn(inception_4e,\n",
        "                                   layer='inception_5a_3x3',\n",
        "                                   cv1_out=96,\n",
        "                                   cv1_filter=(1, 1),\n",
        "                                   cv2_out=384,\n",
        "                                   cv2_filter=(3, 3),\n",
        "                                   cv2_strides=(1, 1),\n",
        "                                   padding=(1, 1))\n",
        "\n",
        "inception_5a_pool = Lambda(lambda x: x**2, name='power2_5a')(inception_4e)\n",
        "inception_5a_pool = AveragePooling2D(pool_size=(3, 3), strides=(3, 3))(inception_5a_pool)\n",
        "inception_5a_pool = Lambda(lambda x: x*9, name='mult9_5a')(inception_5a_pool)\n",
        "inception_5a_pool = Lambda(lambda x: K.sqrt(x), name='sqrt_5a')(inception_5a_pool)\n",
        "inception_5a_pool = utils.conv2d_bn(inception_5a_pool,\n",
        "                                   layer='inception_5a_pool',\n",
        "                                   cv1_out=96,\n",
        "                                   cv1_filter=(1, 1),\n",
        "                                   padding=(1, 1))\n",
        "inception_5a_1x1 = utils.conv2d_bn(inception_4e,\n",
        "                                   layer='inception_5a_1x1',\n",
        "                                   cv1_out=256,\n",
        "                                   cv1_filter=(1, 1))\n",
        "\n",
        "inception_5a = concatenate([inception_5a_3x3, inception_5a_pool, inception_5a_1x1], axis=3)\n",
        "\n",
        "#inception_5b\n",
        "inception_5b_3x3 = utils.conv2d_bn(inception_5a,\n",
        "                                   layer='inception_5b_3x3',\n",
        "                                   cv1_out=96,\n",
        "                                   cv1_filter=(1, 1),\n",
        "                                   cv2_out=384,\n",
        "                                   cv2_filter=(3, 3),\n",
        "                                   cv2_strides=(1, 1),\n",
        "                                   padding=(1, 1))\n",
        "inception_5b_pool = MaxPooling2D(pool_size=3, strides=2)(inception_5a)\n",
        "inception_5b_pool = utils.conv2d_bn(inception_5b_pool,\n",
        "                                   layer='inception_5b_pool',\n",
        "                                   cv1_out=96,\n",
        "                                   cv1_filter=(1, 1))\n",
        "inception_5b_pool = ZeroPadding2D(padding=(1, 1))(inception_5b_pool)\n",
        "\n",
        "inception_5b_1x1 = utils.conv2d_bn(inception_5a,\n",
        "                                   layer='inception_5b_1x1',\n",
        "                                   cv1_out=256,\n",
        "                                   cv1_filter=(1, 1))\n",
        "inception_5b = concatenate([inception_5b_3x3, inception_5b_pool, inception_5b_1x1], axis=3)\n",
        "\n",
        "av_pool = AveragePooling2D(pool_size=(3, 3), strides=(1, 1))(inception_5b)\n",
        "reshape_layer = Flatten()(av_pool)\n",
        "dense_layer = Dense(128, name='dense_layer')(reshape_layer)\n",
        "norm_layer = Lambda(lambda  x: K.l2_normalize(x, axis=1), name='norm_layer')(dense_layer)\n",
        "\n",
        "\n",
        "# Final Model\n",
        "model = Model(inputs=[myInput], outputs=norm_layer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DK6XjIumkK0J"
      },
      "source": [
        "# Load weights from csv files (which was exported from Openface torch model)\n",
        "weights = utils.weights\n",
        "weights_dict = utils.load_weights()\n",
        "\n",
        "# Set layer weights of the model\n",
        "for name in weights:\n",
        "  if model.get_layer(name) != None:\n",
        "    model.get_layer(name).set_weights(weights_dict[name])\n",
        "  elif model.get_layer(name) != None:\n",
        "    model.get_layer(name).set_weights(weights_dict[name])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVCkcwzqkMnx"
      },
      "source": [
        "def image_to_embedding(image, model):\n",
        "    #image = cv2.resize(image, (96, 96), interpolation=cv2.INTER_AREA) \n",
        "    image = cv2.resize(image, (96, 96)) \n",
        "    img = image[...,::-1]\n",
        "    img = np.around(np.transpose(img, (0,1,2))/255.0, decimals=12)\n",
        "    x_train = np.array([img])\n",
        "    embedding = model.predict_on_batch(x_train)\n",
        "    return embedding"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePqA-fq9kOFR"
      },
      "source": [
        "def recognize_face(face_image, input_embeddings, model):\n",
        "\n",
        "    embedding = image_to_embedding(face_image, model)\n",
        "    \n",
        "    minimum_distance = 200\n",
        "    name = None\n",
        "    \n",
        "    # Loop over  names and encodings.\n",
        "    for (input_name, input_embedding) in input_embeddings.items():\n",
        "        \n",
        "       \n",
        "        euclidean_distance = np.linalg.norm(embedding-input_embedding)\n",
        "        \n",
        "\n",
        "        print('Euclidean distance from %s is %s' %(input_name.split('_')[0], euclidean_distance))\n",
        "\n",
        "        \n",
        "        if euclidean_distance < minimum_distance:\n",
        "            minimum_distance = euclidean_distance\n",
        "            name = input_name\n",
        "    \n",
        "    if minimum_distance < 0.8:\n",
        "        return str(name)\n",
        "    else:\n",
        "        return None"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}